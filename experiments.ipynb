{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0882e31a",
   "metadata": {},
   "source": [
    "# Experiments Notebook\n",
    "\n",
    "### This Jupyter Notebook contains all executed experiments and allows for reproduction of the results.\n",
    "\n",
    "<span style=\"color:#FFC000\">Note: Make sure you have completed setting up the development environment, as described in the *README.md*, as well as prepared the dataset by walking through the *setup.ipynb* notebook. Then make sure to select the local venv as kernel.</span>.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21766413",
   "metadata": {},
   "source": [
    "### Seed Generation\n",
    "\n",
    "The script below will generate n seeds using secure random number generation and md5 hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c7cd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4140620135\n",
      "1132905197\n",
      "3754523883\n",
      "3492508408\n",
      "3378715402\n",
      "4213948446\n",
      "1521997286\n",
      "1377995568\n",
      "1449566991\n"
     ]
    }
   ],
   "source": [
    "import secrets\n",
    "import hashlib\n",
    "\n",
    "n = 7\n",
    "\n",
    "for i in range(n):\n",
    "    randnum = secrets.randbelow(100_000_000_000)\n",
    "    seed_hex = hashlib.md5(str(randnum).encode()).hexdigest()\n",
    "    seed_int = int(seed_hex, 16) % 2**32 - 1\n",
    "    print(seed_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39b991a",
   "metadata": {},
   "source": [
    "### Generated Seeds\n",
    "\n",
    "The following cell allows reproducability of the experiments, showing the seeds used for the showcased runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fead7d1",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "The following cells allows configuration of the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8781e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEEDS = [    \n",
    "4140620135, # StratifiedShuffleSplit\n",
    "1132905197, # RepeatedStratifiedKFold\n",
    "3754523883, # |\n",
    "3492508408, # |\n",
    "3378715402, # Model Seeds\n",
    "4213948446, # |\n",
    "1521997286, # |\n",
    "]\n",
    "\n",
    "EXPERIMENT_NAMES = [\n",
    "    \"VISIBLE\",\n",
    "    \"INFRARED\",\n",
    "    \"VISIBLE_INFRARED\"\n",
    "]\n",
    "\n",
    "# RSKF\n",
    "N_SPLITS = 5\n",
    "N_REPEATS = 5\n",
    "\n",
    "FOLD_SIZE = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fec8aee",
   "metadata": {},
   "source": [
    "planned order of notebook:\n",
    "1. one autorun (runs all three with verbose=False or True. if false only print results. this will take days!)\n",
    "2. experiment execution! here do one by one. viisble, then infrared, then mixed. for each the same function should be called but with different args, here code should be simple and short\n",
    "3. statistical analysis. laod experiment results and show significance\n",
    "4. if significant, run the paired wilcoxon test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dde61a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8896beb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sequences: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 318/318 [00:09<00:00, 33.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X shape: (296901,)\n",
      "Original stratification labels: (array([0, 1]), array([ 16683, 280218]))\n",
      "Sampled X shape: (1000,)\n",
      "Sampled stratification labels: (array([0, 1]), array([ 56, 944]))\n",
      "Saved RSKF config to experiments/rskf_splits.npy.\n",
      "(25,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from uav.experiments.create import create_experiments\n",
    "\n",
    "create_experiments(\n",
    "    seeds=SEEDS[:2],\n",
    "    source_dir=\"datasets/anti-uav300\",\n",
    "    fold_size=FOLD_SIZE,\n",
    "    n_splits=N_SPLITS, \n",
    "    n_repeats=N_REPEATS,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a34ecd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 0\n",
      "Running vz-3754523883 ...\n",
      "New https://pypi.org/project/ultralytics/8.3.228 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.219 ðŸš€ Python-3.13.5 torch-2.9.0 MPS (Apple M4)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/var/folders/xw/45g67_dd7glf1l755cb4gtgc0000gn/T/tmp3d04kmk2/cfg.yaml, degrees=0.0, deterministic=True, device=mps, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=2, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=models/weights/yolo12n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=vz-3754523883-f_0_train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=results/exp_train_runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/Users/julian/workspace/pwr/ai/results/exp_train_runs/vz-3754523883-f_0_train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=3754523883, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  2    180864  ultralytics.nn.modules.block.A2C2f           [128, 128, 2, True, 4]        \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  2    689408  ultralytics.nn.modules.block.A2C2f           [256, 256, 2, True, 1]        \n",
      "  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 11                  -1  1     86912  ultralytics.nn.modules.block.A2C2f           [384, 128, 1, False, -1]      \n",
      " 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 14                  -1  1     24000  ultralytics.nn.modules.block.A2C2f           [256, 64, 1, False, -1]       \n",
      " 15                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1     74624  ultralytics.nn.modules.block.A2C2f           [192, 128, 1, False, -1]      \n",
      " 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 21        [14, 17, 20]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLOv12n summary: 272 layers, 2,568,243 parameters, 2,568,227 gradients, 6.5 GFLOPs\n",
      "\n",
      "Transferred 640/691 items from pretrained weights\n",
      "Freezing layer 'model.21.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 4468.6Â±1519.5 MB/s, size: 98.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /private/var/folders/xw/45g67_dd7glf1l755cb4gtgc0000gn/T/tmp3d04kmk2/train/labels... 800 images, 45 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 800/800 6.6Kit/s 0.1s0.1s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /private/var/folders/xw/45g67_dd7glf1l755cb4gtgc0000gn/T/tmp3d04kmk2/train/labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 4533.7Â±1265.3 MB/s, size: 107.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /private/var/folders/xw/45g67_dd7glf1l755cb4gtgc0000gn/T/tmp3d04kmk2/val/labels... 200 images, 11 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 200/200 6.6Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /private/var/folders/xw/45g67_dd7glf1l755cb4gtgc0000gn/T/tmp3d04kmk2/val/labels.cache\n",
      "Plotting labels to /Users/julian/workspace/pwr/ai/results/exp_train_runs/vz-3754523883-f_0_train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 113 weight(decay=0.0), 120 weight(decay=0.0005), 119 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/julian/workspace/pwr/ai/results/exp_train_runs/vz-3754523883-f_0_train\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        1/2      6.34G      1.774      3.056      1.183         30        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 50/50 0.9it/s 55.3s1.0ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 1.0it/s 6.9s1.0ss\n",
      "                   all        200        189    0.00293      0.931    0.00847    0.00342\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        2/2      6.34G       1.63      1.977      1.155         32        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 50/50 0.9it/s 54.1s1.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 0.6it/s 12.1s2.2s\n",
      "                   all        200        189      0.134      0.841      0.176     0.0961\n",
      "\n",
      "2 epochs completed in 0.036 hours.\n",
      "Optimizer stripped from /Users/julian/workspace/pwr/ai/results/exp_train_runs/vz-3754523883-f_0_train/weights/last.pt, 5.5MB\n",
      "Optimizer stripped from /Users/julian/workspace/pwr/ai/results/exp_train_runs/vz-3754523883-f_0_train/weights/best.pt, 5.5MB\n",
      "\n",
      "Validating /Users/julian/workspace/pwr/ai/results/exp_train_runs/vz-3754523883-f_0_train/weights/best.pt...\n",
      "Ultralytics 8.3.219 ðŸš€ Python-3.13.5 torch-2.9.0 MPS (Apple M4)\n",
      "YOLOv12n summary (fused): 159 layers, 2,556,923 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 1.0it/s 7.0s1.2ss\n",
      "                   all        200        189      0.134      0.841      0.176     0.0963\n",
      "Speed: 0.1ms preprocess, 8.0ms inference, 0.0ms loss, 16.5ms postprocess per image\n",
      "Results saved to \u001b[1m/Users/julian/workspace/pwr/ai/results/exp_train_runs/vz-3754523883-f_0_train\u001b[0m\n",
      "Ultralytics 8.3.219 ðŸš€ Python-3.13.5 torch-2.9.0 MPS (Apple M4)\n",
      "YOLOv12n summary (fused): 159 layers, 2,556,923 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 3337.9Â±988.9 MB/s, size: 105.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /private/var/folders/xw/45g67_dd7glf1l755cb4gtgc0000gn/T/tmp3d04kmk2/val/labels.cache... 200 images, 11 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 200/200 1.1Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 2.9it/s 4.5s0.3s\n",
      "                   all        200        189      0.134      0.841      0.176     0.0963\n",
      "Speed: 0.2ms preprocess, 6.2ms inference, 0.0ms loss, 5.7ms postprocess per image\n",
      "Saving /Users/julian/workspace/pwr/ai/results/exp_train_runs/vz-3754523883-f_0_val/predictions.json...\n",
      "Results saved to \u001b[1m/Users/julian/workspace/pwr/ai/results/exp_train_runs/vz-3754523883-f_0_val\u001b[0m\n",
      "Processing fold 1\n",
      "Running vz-3754523883 ...\n",
      "New https://pypi.org/project/ultralytics/8.3.228 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.219 ðŸš€ Python-3.13.5 torch-2.9.0 MPS (Apple M4)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/var/folders/xw/45g67_dd7glf1l755cb4gtgc0000gn/T/tmphu2gdf2m/cfg.yaml, degrees=0.0, deterministic=True, device=mps, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=2, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=models/weights/yolo12n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=vz-3754523883-f_1_train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=results/exp_train_runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/Users/julian/workspace/pwr/ai/results/exp_train_runs/vz-3754523883-f_1_train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=3754523883, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  2    180864  ultralytics.nn.modules.block.A2C2f           [128, 128, 2, True, 4]        \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  2    689408  ultralytics.nn.modules.block.A2C2f           [256, 256, 2, True, 1]        \n",
      "  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 11                  -1  1     86912  ultralytics.nn.modules.block.A2C2f           [384, 128, 1, False, -1]      \n",
      " 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 14                  -1  1     24000  ultralytics.nn.modules.block.A2C2f           [256, 64, 1, False, -1]       \n",
      " 15                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1     74624  ultralytics.nn.modules.block.A2C2f           [192, 128, 1, False, -1]      \n",
      " 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 21        [14, 17, 20]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLOv12n summary: 272 layers, 2,568,243 parameters, 2,568,227 gradients, 6.5 GFLOPs\n",
      "\n",
      "Transferred 640/691 items from pretrained weights\n",
      "Freezing layer 'model.21.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2905.0Â±579.2 MB/s, size: 95.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /private/var/folders/xw/45g67_dd7glf1l755cb4gtgc0000gn/T/tmphu2gdf2m/train/labels... 800 images, 45 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 800/800 6.6Kit/s 0.1s0.1s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /private/var/folders/xw/45g67_dd7glf1l755cb4gtgc0000gn/T/tmphu2gdf2m/train/labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 3420.6Â±1016.4 MB/s, size: 116.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /private/var/folders/xw/45g67_dd7glf1l755cb4gtgc0000gn/T/tmphu2gdf2m/val/labels... 200 images, 11 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 200/200 5.9Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /private/var/folders/xw/45g67_dd7glf1l755cb4gtgc0000gn/T/tmphu2gdf2m/val/labels.cache\n",
      "Plotting labels to /Users/julian/workspace/pwr/ai/results/exp_train_runs/vz-3754523883-f_1_train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 113 weight(decay=0.0), 120 weight(decay=0.0005), 119 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/julian/workspace/pwr/ai/results/exp_train_runs/vz-3754523883-f_1_train\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        1/2      6.42G      1.739      2.971      1.169         25        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 50/50 0.9it/s 55.3s1.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 1.1it/s 6.4s1.1ss\n",
      "                   all        200        189     0.0027      0.857    0.00649    0.00208\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        2/2      6.37G      1.654          2      1.156         28        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 50/50 0.9it/s 54.1s1.0ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 1.2it/s 6.1s1.0ss\n",
      "                   all        200        189    0.00298      0.947      0.373      0.159\n",
      "\n",
      "2 epochs completed in 0.034 hours.\n",
      "Optimizer stripped from /Users/julian/workspace/pwr/ai/results/exp_train_runs/vz-3754523883-f_1_train/weights/last.pt, 5.5MB\n",
      "Optimizer stripped from /Users/julian/workspace/pwr/ai/results/exp_train_runs/vz-3754523883-f_1_train/weights/best.pt, 5.5MB\n",
      "\n",
      "Validating /Users/julian/workspace/pwr/ai/results/exp_train_runs/vz-3754523883-f_1_train/weights/best.pt...\n",
      "Ultralytics 8.3.219 ðŸš€ Python-3.13.5 torch-2.9.0 MPS (Apple M4)\n",
      "YOLOv12n summary (fused): 159 layers, 2,556,923 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 1.5it/s 4.8s0.9ss\n",
      "                   all        200        189    0.00298      0.947      0.373       0.16\n",
      "Speed: 0.1ms preprocess, 6.1ms inference, 0.0ms loss, 8.0ms postprocess per image\n",
      "Results saved to \u001b[1m/Users/julian/workspace/pwr/ai/results/exp_train_runs/vz-3754523883-f_1_train\u001b[0m\n",
      "Ultralytics 8.3.219 ðŸš€ Python-3.13.5 torch-2.9.0 MPS (Apple M4)\n",
      "YOLOv12n summary (fused): 159 layers, 2,556,923 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2658.0Â±888.6 MB/s, size: 122.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /private/var/folders/xw/45g67_dd7glf1l755cb4gtgc0000gn/T/tmphu2gdf2m/val/labels.cache... 200 images, 11 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 200/200 939.4Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.1it/s 4.2s0.3s\n",
      "                   all        200        189    0.00298      0.947      0.373       0.16\n",
      "Speed: 0.1ms preprocess, 4.7ms inference, 0.0ms loss, 5.5ms postprocess per image\n",
      "Saving /Users/julian/workspace/pwr/ai/results/exp_train_runs/vz-3754523883-f_1_val/predictions.json...\n",
      "Results saved to \u001b[1m/Users/julian/workspace/pwr/ai/results/exp_train_runs/vz-3754523883-f_1_val\u001b[0m\n",
      "Processing fold 2\n",
      "Running vz-3754523883 ...\n",
      "New https://pypi.org/project/ultralytics/8.3.228 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.219 ðŸš€ Python-3.13.5 torch-2.9.0 MPS (Apple M4)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/var/folders/xw/45g67_dd7glf1l755cb4gtgc0000gn/T/tmp4bw_qyea/cfg.yaml, degrees=0.0, deterministic=True, device=mps, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=2, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=models/weights/yolo12n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=vz-3754523883-f_2_train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=results/exp_train_runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/Users/julian/workspace/pwr/ai/results/exp_train_runs/vz-3754523883-f_2_train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=3754523883, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  2    180864  ultralytics.nn.modules.block.A2C2f           [128, 128, 2, True, 4]        \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  2    689408  ultralytics.nn.modules.block.A2C2f           [256, 256, 2, True, 1]        \n",
      "  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 11                  -1  1     86912  ultralytics.nn.modules.block.A2C2f           [384, 128, 1, False, -1]      \n",
      " 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 14                  -1  1     24000  ultralytics.nn.modules.block.A2C2f           [256, 64, 1, False, -1]       \n",
      " 15                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1     74624  ultralytics.nn.modules.block.A2C2f           [192, 128, 1, False, -1]      \n",
      " 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 21        [14, 17, 20]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLOv12n summary: 272 layers, 2,568,243 parameters, 2,568,227 gradients, 6.5 GFLOPs\n",
      "\n",
      "Transferred 640/691 items from pretrained weights\n",
      "Freezing layer 'model.21.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 3659.7Â±1041.6 MB/s, size: 98.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /private/var/folders/xw/45g67_dd7glf1l755cb4gtgc0000gn/T/tmp4bw_qyea/train/labels... 800 images, 45 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 800/800 6.5Kit/s 0.1s0.1s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /private/var/folders/xw/45g67_dd7glf1l755cb4gtgc0000gn/T/tmp4bw_qyea/train/labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 3905.6Â±1187.8 MB/s, size: 110.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /private/var/folders/xw/45g67_dd7glf1l755cb4gtgc0000gn/T/tmp4bw_qyea/val/labels... 200 images, 11 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 200/200 7.4Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /private/var/folders/xw/45g67_dd7glf1l755cb4gtgc0000gn/T/tmp4bw_qyea/val/labels.cache\n",
      "Plotting labels to /Users/julian/workspace/pwr/ai/results/exp_train_runs/vz-3754523883-f_2_train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 113 weight(decay=0.0), 120 weight(decay=0.0005), 119 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/julian/workspace/pwr/ai/results/exp_train_runs/vz-3754523883-f_2_train\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        1/2      6.43G      1.789      3.374      1.188         19        640: 64% â”â”â”â”â”â”â”â•¸â”€â”€â”€â”€ 32/50 1.3it/s 36.7s<14.1s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      3\u001b[39m SEEDS = [    \n\u001b[32m      4\u001b[39m \u001b[32m4140620135\u001b[39m, \u001b[38;5;66;03m# StratifiedShuffleSplit\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[32m1132905197\u001b[39m, \u001b[38;5;66;03m# RepeatedStratifiedKFold\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m \u001b[32m1521997286\u001b[39m, \u001b[38;5;66;03m# |\u001b[39;00m\n\u001b[32m     11\u001b[39m ]\n\u001b[32m     13\u001b[39m N_SPLITS = \u001b[32m5\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mrun_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN_SPLITS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSEEDS\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[32;43m7\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/pwr/ai/uav/experiments/run.py:90\u001b[39m, in \u001b[36mrun_experiments\u001b[39m\u001b[34m(n_splits, model_seeds)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_seed \u001b[38;5;129;01min\u001b[39;00m model_seeds:\n\u001b[32m     89\u001b[39m     experiment_name = \u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (modality.value, model_seed)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_results\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_repeated_k_fold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_seed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m        \u001b[49m\u001b[43mappend_results\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresults/metrics.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_results\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/pwr/ai/uav/experiments/run.py:40\u001b[39m, in \u001b[36mrun_repeated_k_fold\u001b[39m\u001b[34m(model_seed, n_splits, splits, experiment_name)\u001b[39m\n\u001b[32m     37\u001b[39m model = YOLO(\u001b[33m\"\u001b[39m\u001b[33mmodels/weights/yolo12n.pt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning\u001b[39m\u001b[33m\"\u001b[39m, experiment_name, \u001b[33m\"\u001b[39m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpname_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresults/exp_train_runs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m model.val(\n\u001b[32m     51\u001b[39m     data=cfg,\n\u001b[32m     52\u001b[39m     imgsz=\u001b[32m640\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     57\u001b[39m     save_json=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     58\u001b[39m )\n\u001b[32m     60\u001b[39m df = pd.read_csv(os.path.join(tpath, \u001b[33m\"\u001b[39m\u001b[33mresults.csv\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/pwr/ai/models/ultralytics/ultralytics/engine/model.py:800\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    797\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainer.model = \u001b[38;5;28mself\u001b[39m.trainer.get_model(weights=\u001b[38;5;28mself\u001b[39m.model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg=\u001b[38;5;28mself\u001b[39m.model.yaml)\n\u001b[32m    798\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m--> \u001b[39m\u001b[32m800\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    802\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/pwr/ai/models/ultralytics/ultralytics/engine/trainer.py:238\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    235\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/pwr/ai/models/ultralytics/ultralytics/engine/trainer.py:422\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    420\u001b[39m     loss, \u001b[38;5;28mself\u001b[39m.loss_items = unwrap_model(\u001b[38;5;28mself\u001b[39m.model).loss(batch, preds)\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m     loss, \u001b[38;5;28mself\u001b[39m.loss_items = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[38;5;28mself\u001b[39m.loss = loss.sum()\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK != -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/pwr/ai/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/pwr/ai/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/pwr/ai/models/ultralytics/ultralytics/nn/tasks.py:138\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[33;03mPerform forward pass of the model for either training or inference.\u001b[39;00m\n\u001b[32m    126\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m    (torch.Tensor): Loss if x is a dict (training), or network predictions (inference).\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predict(x, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/pwr/ai/models/ultralytics/ultralytics/nn/tasks.py:339\u001b[39m, in \u001b[36mBaseModel.loss\u001b[39m\u001b[34m(self, batch, preds)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    338\u001b[39m     preds = \u001b[38;5;28mself\u001b[39m.forward(batch[\u001b[33m\"\u001b[39m\u001b[33mimg\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/pwr/ai/models/ultralytics/ultralytics/utils/loss.py:272\u001b[39m, in \u001b[36mv8DetectionLoss.__call__\u001b[39m\u001b[34m(self, preds, batch)\u001b[39m\n\u001b[32m    268\u001b[39m pred_bboxes = \u001b[38;5;28mself\u001b[39m.bbox_decode(anchor_points, pred_distri)  \u001b[38;5;66;03m# xyxy, (b, h*w, 4)\u001b[39;00m\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# dfl_conf = pred_distri.view(batch_size, -1, 4, self.reg_max).detach().softmax(-1)\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[38;5;66;03m# dfl_conf = (dfl_conf.amax(-1).mean(-1) + dfl_conf.amax(-1).amin(-1)) / 2\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m _, target_bboxes, target_scores, fg_mask, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massigner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# pred_scores.detach().sigmoid() * 0.8 + dfl_conf.unsqueeze(-1) * 0.2,\u001b[39;49;00m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpred_scores\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_bboxes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43manchor_points\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_gt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m target_scores_sum = \u001b[38;5;28mmax\u001b[39m(target_scores.sum(), \u001b[32m1\u001b[39m)\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# Cls loss\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[38;5;66;03m# loss[1] = self.varifocal_loss(pred_scores, target_scores, target_labels) / target_scores_sum  # VFL way\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/pwr/ai/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/pwr/ai/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/pwr/ai/venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/pwr/ai/models/ultralytics/ultralytics/utils/tal.py:82\u001b[39m, in \u001b[36mTaskAlignedAssigner.forward\u001b[39m\u001b[34m(self, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt)\u001b[39m\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m     74\u001b[39m         torch.full_like(pd_scores[..., \u001b[32m0\u001b[39m], \u001b[38;5;28mself\u001b[39m.num_classes),\n\u001b[32m     75\u001b[39m         torch.zeros_like(pd_bboxes),\n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m         torch.zeros_like(pd_scores[..., \u001b[32m0\u001b[39m]),\n\u001b[32m     79\u001b[39m     )\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manc_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_gt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m torch.cuda.OutOfMemoryError:\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# Move tensors to CPU, compute, then move back to original device\u001b[39;00m\n\u001b[32m     85\u001b[39m     LOGGER.warning(\u001b[33m\"\u001b[39m\u001b[33mCUDA OutOfMemoryError in TaskAlignedAssigner, using CPU\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/pwr/ai/models/ultralytics/ultralytics/utils/tal.py:109\u001b[39m, in \u001b[36mTaskAlignedAssigner._forward\u001b[39m\u001b[34m(self, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt):\n\u001b[32m     91\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[33;03m    Compute the task-aligned assignment.\u001b[39;00m\n\u001b[32m     93\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m \u001b[33;03m        target_gt_idx (torch.Tensor): Target ground truth indices with shape (bs, num_total_anchors).\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     mask_pos, align_metric, overlaps = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_pos_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpd_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manc_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_gt\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m     target_gt_idx, fg_mask, mask_pos = \u001b[38;5;28mself\u001b[39m.select_highest_overlaps(mask_pos, overlaps, \u001b[38;5;28mself\u001b[39m.n_max_boxes)\n\u001b[32m    115\u001b[39m     \u001b[38;5;66;03m# Assigned target\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/pwr/ai/models/ultralytics/ultralytics/utils/tal.py:146\u001b[39m, in \u001b[36mTaskAlignedAssigner.get_pos_mask\u001b[39m\u001b[34m(self, pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points, mask_gt)\u001b[39m\n\u001b[32m    144\u001b[39m mask_in_gts = \u001b[38;5;28mself\u001b[39m.select_candidates_in_gts(anc_points, gt_bboxes)\n\u001b[32m    145\u001b[39m \u001b[38;5;66;03m# Get anchor_align metric, (b, max_num_obj, h*w)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m align_metric, overlaps = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_box_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_in_gts\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_gt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[38;5;66;03m# Get topk_metric mask, (b, max_num_obj, h*w)\u001b[39;00m\n\u001b[32m    148\u001b[39m mask_topk = \u001b[38;5;28mself\u001b[39m.select_topk_candidates(align_metric, topk_mask=mask_gt.expand(-\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.topk).bool())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/pwr/ai/models/ultralytics/ultralytics/utils/tal.py:181\u001b[39m, in \u001b[36mTaskAlignedAssigner.get_box_metrics\u001b[39m\u001b[34m(self, pd_scores, pd_bboxes, gt_labels, gt_bboxes, mask_gt)\u001b[39m\n\u001b[32m    178\u001b[39m bbox_scores[mask_gt] = pd_scores[ind[\u001b[32m0\u001b[39m], :, ind[\u001b[32m1\u001b[39m]][mask_gt]  \u001b[38;5;66;03m# b, max_num_obj, h*w\u001b[39;00m\n\u001b[32m    180\u001b[39m \u001b[38;5;66;03m# (b, max_num_obj, 1, 4), (b, 1, h*w, 4)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m pd_boxes = \u001b[43mpd_bboxes\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m.expand(-\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.n_max_boxes, -\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)[mask_gt]\n\u001b[32m    182\u001b[39m gt_boxes = gt_bboxes.unsqueeze(\u001b[32m2\u001b[39m).expand(-\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m, na, -\u001b[32m1\u001b[39m)[mask_gt]\n\u001b[32m    183\u001b[39m overlaps[mask_gt] = \u001b[38;5;28mself\u001b[39m.iou_calculation(gt_boxes, pd_boxes)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from uav.experiments.run import run_experiments\n",
    "\n",
    "SEEDS = [    \n",
    "4140620135, # StratifiedShuffleSplit\n",
    "1132905197, # RepeatedStratifiedKFold\n",
    "3754523883, # |\n",
    "3492508408, # |\n",
    "3378715402, # Model Seeds\n",
    "4213948446, # |\n",
    "1521997286, # |\n",
    "]\n",
    "\n",
    "N_SPLITS = 5\n",
    "\n",
    "run_experiments(N_SPLITS, SEEDS[2:7])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
